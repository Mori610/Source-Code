import pandas as pd
import numpy as np
import warnings
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
import lightgbm as lgb
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import LabelEncoder
warnings.simplefilter('ignore')
train_columns = None
test_columns  = None

def reduce_mem_usage(props):
    start_mem_usg = props.memory_usage().sum() / 1024**2 
    #print("Memory usage of properties dataframe is :",start_mem_usg," MB")
    NAlist = [] # Keeps track of columns that have missing values filled in. 
    for col in props.columns:
        if props[col].dtype != object:  # Exclude strings
            
            # Print current column type
            #print("******************************")
            #print("Column: ",col)
            #print("dtype before: ",props[col].dtype)
            
            # make variables for Int, max and min
            IsInt = False
            mx = props[col].max()
            mn = props[col].min()
            
            # Integer does not support NA, therefore, NA needs to be filled
            if not np.isfinite(props[col]).all(): 
                NAlist.append(col)
                props[col].fillna(mn-1,inplace=True)  
                   
            # test if column can be converted to an integer
            asint = props[col].fillna(0).astype(np.int64)
            result = (props[col] - asint)
            result = result.sum()
            if result > -0.01 and result < 0.01:
                IsInt = True

            
            # Make Integer/unsigned Integer datatypes
            if IsInt:
                if mn >= 0:
                    if mx < 255:
                        props[col] = props[col].astype(np.uint8)
                    elif mx < 65535:
                        props[col] = props[col].astype(np.uint16)
                    elif mx < 4294967295:
                        props[col] = props[col].astype(np.uint32)
                    else:
                        props[col] = props[col].astype(np.uint64)
                else:
                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:
                        props[col] = props[col].astype(np.int8)
                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:
                        props[col] = props[col].astype(np.int16)
                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:
                        props[col] = props[col].astype(np.int32)
                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:
                        props[col] = props[col].astype(np.int64)    
            
            # Make float datatypes 32 bit
            else:
                props[col] = props[col].astype(np.float32)
            
            # Print new column type
            #print("dtype after: ",props[col].dtype)
            #print("******************************")
    
    # Print final result
    #print("___MEMORY USAGE AFTER COMPLETION:___")
    mem_usg = props.memory_usage().sum() / 1024**2 
   #print("Memory usage is: ",mem_usg," MB")
    #print("This is ",100*mem_usg/start_mem_usg,"% of the initial size")
    return props, NAlist

#Generator作成
def train_Generator():
    global train_columns
    purchase_df = pd.read_csv("AIQuest/purchase_record.csv")
    user_df = pd.read_csv("AIQuest/user_info.csv")
    purchase_df.fillna(0,inplace = True)
    user_df.fillna(0,inplace = True)
    train_df = pd.merge(purchase_df, user_df, how='left', on='user_id')
    train_df, NAlist = reduce_mem_usage(train_df)
    train_columns = train_df.columns
    train_df = train_df.to_numpy()
    x = len(train_df)//500
    output = []
    for idx, b in enumerate(train_df):
        output.append(b)
        a = len(train_df)-idx
        if len(output) == x:
            yield pd.DataFrame(output)
            output = []
        elif a == (x*500- len(train_df)-1):
            yield pd.DataFrame(output)
            output =[]

def test_Generator():
    global test_columns
    test_df = pd.read_csv("AIQuest/purchase_record_test.csv")
    user_df = pd.read_csv("AIQuest/user_info.csv")
    test_df.fillna(0,inplace = True)
    user_df.fillna(0,inplace = True)
    test_df = pd.merge(test_df, user_df, how='left', on='user_id')
    test_df, NAlist = reduce_mem_usage(test_df)
    test_columns = test_df.columns
    test_df = test_df.to_numpy()
    x = len(test_df)//60
    output = []
    for idx, b in enumerate(test_df):
        output.append(b)
        a = len(test_df)-idx
        if len(output) == x:
            yield pd.DataFrame(output)
            output = []
        elif a == (x*500- len(test_df)-1):
            yield pd.DataFrame(output)
            output =[]
            
#カラム数の調整
def fill_missing_columns(df_a, df_b):
    columns_for_b = set(df_a.columns) - set(df_b.columns)
    for column in columns_for_b:
        df_b[column] = 0
    columns_for_a = set(df_b.columns) - set(df_a.columns)
    for column in columns_for_a:
        df_a[column] = 0

        
# 学習データを読み込み 
#train_gen = train_Generator()
test_gen = test_Generator()
logreg = LogisticRegression()
for idx ,i in enumerate(train_Generator()):
    #train_df = next(train_gen)
    train_df = i
    train_df.columns = train_columns
    print(train_df.shape)
    train_df = train_df.drop('purchase_id',axis=1)
    train = pd.get_dummies(train_df,drop_first=True)
    train = train.drop('purchase',axis = 1)
    #print(train.dtypes)  
    X = train
    y = train_df.purchase
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33,random_state = 0)
    logreg.fit(X_train, y_train)
    print(logreg.score(X_test,y_test))
    print("Learning End"+ str(idx))
    
print("Learning Complete")
sample_test_df = {'purchase_id':0,'Probability':0}
for idx,a in enumerate(test_gen):
    test_df = a
    #test_df = next(test_gen)
    test_df.columns = test_columns
    test = pd.get_dummies(test_df,drop_first=True)
    columns_for_b = set(test.columns) - set(train.columns)
    proba = logreg.predict_proba(test)[:, 1]
    sample_test_df['purchase_id'].extend(test_df['purchase_id'])
    sample_test_df['Probability'].extend(proba)
    print("Complete")

# 提出
submit_df = sample_test_df[['purchase_id', 'probability']]
submit_df.to_csv('AIQuest/submit_191125.csv', header=False, index=False, )
